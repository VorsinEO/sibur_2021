{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence \n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    StandardScaler,\n",
    "    MaxAbsScaler,\n",
    "    RobustScaler,\n",
    "    QuantileTransformer,\n",
    "    PowerTransformer,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error as msle\n",
    "from tqdm import tqdm\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Clamp(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input.clamp(min=0, max=5000)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.clone()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class MSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        return self.mse(torch.log(pred + 1), torch.log(actual + 1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Загружу данные и гляну наних"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "DATA_DIR = pathlib.Path(\"/Users/ltorrick/DS/sibur_2021/data\")\n",
    "HASH2IND_PATH = pathlib.Path('/Users/ltorrick/DS/sibur_2021/code/rnn_/hash2ind.pickle')\n",
    "ENCODERS_PATH = pathlib.Path('/Users/ltorrick/DS/sibur_2021/code/rnn_/encoders.pickle')\n",
    "#HASH2IND_PATH = pathlib.Path(__file__).parent.joinpath(\"hash2ind.pickle\")\n",
    "#ENCODERS_PATH = pathlib.Path(__file__).parent.joinpath(\"encoders.pickle\")\n",
    "#DATA_DIR = pathlib.Path(\".\")\n",
    "DATA_FILE = \"sc2021_train_deals.csv\"\n",
    "AGG_COLS = [\"material_code\", \"company_code\", \"country\", \"region\", \"manager_code\"]\n",
    "#RS = 82736\n",
    "#PATH_TO_ENCODER_WEIGHTS = pathlib.Path(__file__).parent.joinpath('encoder_w.pt')\n",
    "#PATH_TO_RNN_WEIGHTS = pathlib.Path(__file__).parent.joinpath('RNN_w.pt')\n",
    "#PATH_TO_FF_WEIGHTS = pathlib.Path(__file__).parent.joinpath('FF_w.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data = pd.read_csv(DATA_DIR.joinpath(DATA_FILE), parse_dates=[\"month\", \"date\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Создам энкодер для групп товаров\n",
    "Хочу в разрезе ИД группы товара затем логику get_item выстроить для torch Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#keep this code\n",
    "ids = data[AGG_COLS].drop_duplicates()\n",
    "ids['hash']=ids.material_code.astype(str)+ids.company_code.astype(str)+ids.country.astype(str).str.lower()+ids.region.astype(str).str.lower()+ids.manager_code.astype(str)\n",
    "ids['ids'] = range(0, ids.shape[0])\n",
    "ind2hash=ids.set_index('ids').hash.to_dict()\n",
    "hash2ind = {v:k for k,v in ind2hash.items()}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "with open('hash2ind.pickle', 'wb') as handle:\n",
    "    pickle.dump(hash2ind, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "with open('hash2ind.pickle', 'rb') as handle:\n",
    "    hash2ind = pickle.load(handle)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k_/t189wd911690dznlkp4xwd7h0000gn/T/ipykernel_80809/1185285755.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hash2ind.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mhash2ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def prepare_data(df, hash2ind):\n",
    "    df['hash']=df.material_code.astype(str)+df.company_code.astype(str)+df.country.astype(str).str.lower()+df.region.astype(str).str.lower()+df.manager_code.astype(str)\n",
    "    df['ids'] = df['hash'].map(hash2ind)\n",
    "    df.drop('hash', axis=1, inplace=True)\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "data_p = prepare_data(data, hash2ind)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Создаю энкодеры для кат фичей\n",
    "Буду в будущем их использовать для препроцессинга"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "CAT_FEATS = ['material_code', 'company_code', 'country', 'region', 'manager_code','material_lvl1_name', 'material_lvl2_name',\n",
    "       'material_lvl3_name', 'contract_type']\n",
    "encoders = {}\n",
    "for col in CAT_FEATS:\n",
    "    data[col] = data[col].astype('category')\n",
    "    encoders['ind2'+col]=dict(enumerate(data[col].cat.categories))\n",
    "    encoders[col+'2ind'] = {v:k for k, v in encoders['ind2'+col].items()}\n",
    "encoders['CAT_FEATS'] = CAT_FEATS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "with open('encoders.pickle', 'wb') as handle:\n",
    "    pickle.dump(encoders, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Тут функция для объявления скалера таргета\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def get_scaler(scaler):\n",
    "    scalers = {\n",
    "        \"minmax\": MinMaxScaler,\n",
    "        \"standard\": StandardScaler,\n",
    "        \"maxabs\": MaxAbsScaler,\n",
    "        \"robust\": RobustScaler,\n",
    "        \"quantile\": QuantileTransformer,\n",
    "        \"power\": PowerTransformer,\n",
    "        #'log': LogScaler\n",
    "    }\n",
    "    return scalers.get(scaler.lower())()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def import_pickle( pickle_path):\n",
    "    with open(pickle_path, 'rb') as handle:\n",
    "        res_dict = pickle.load(handle)\n",
    "    return res_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Класс для базовой предобработки данных\n",
    "Можно его же потом научить генерить данные для инференса"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class DataPreparation:\n",
    "    def __init__(self, df, target_scaler=None, path_hash2ind=None,\n",
    "    path_encoders=None\n",
    "    ) -> None:\n",
    "        self.df = df.sort_values('date')\n",
    "        if target_scaler:\n",
    "            self.target_scaler = get_scaler(target_scaler)\n",
    "        self.hash2ind = import_pickle(path_hash2ind)\n",
    "        self.encoders = import_pickle(path_encoders)\n",
    "\n",
    "    def base_preproccess(self, scale_target=False):\n",
    "        \"\"\" Выполняет базовые преобразования над данными \"\"\"\n",
    "        df = self.df.copy()\n",
    "        df = self.prepare_ids(df)\n",
    "        df = self.encode_cat(df)\n",
    "        df = self.add_dt_features(df)\n",
    "        df = self.add_agg_feat(df)\n",
    "        if scale_target is not False:\n",
    "            df['volume'] = self.target_scaler.fit_transform(df['volume'].values.reshape(-1,1))\n",
    "\n",
    "        return df\n",
    "        \n",
    "    def prepare_ids(self, df):\n",
    "        df['hash']=df.material_code.astype(str)+df.company_code.astype(str)+df.country.astype(str).str.lower()+df.region.astype(str).str.lower()+df.manager_code.astype(str)\n",
    "        df['ids'] = df['hash'].map(self.hash2ind)\n",
    "        df.drop('hash', axis=1, inplace=True)\n",
    "        return df\n",
    "    def encode_cat(self, df):\n",
    "        CAT_FEATS = self.encoders['CAT_FEATS']\n",
    "        for col in CAT_FEATS:\n",
    "            #df[col] = df[col].astype('category')\n",
    "            df[col] = df[col].map(self.encoders[col+'2ind']).astype('int')+1\n",
    "        return df\n",
    "    def add_dt_features(self, df):\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month_int'] = df['date'].dt.month\n",
    "        df['day'] = df['date'].dt.day\n",
    "        df['quarter'] = df['date'].dt.quarter\n",
    "        return df\n",
    "    def add_agg_feat(self, df):\n",
    "        df_p_agg_sum = df.groupby(AGG_COLS+['month']).agg({'volume':'sum'}).reset_index()\n",
    "        df_p_agg_sum.rename(columns={'volume':'volume_per_month'}, inplace=True)\n",
    "        df = pd.merge(df, df_p_agg_sum, on = AGG_COLS+['month'], how='left')\n",
    "\n",
    "        return df\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "date_prep = DataPreparation(df=data, target_scaler='quantile', path_hash2ind=HASH2IND_PATH, path_encoders=ENCODERS_PATH)\n",
    "df_p = date_prep.base_preproccess(scale_target=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "df_p.head(2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>material_code</th>\n",
       "      <th>company_code</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>manager_code</th>\n",
       "      <th>month</th>\n",
       "      <th>material_lvl1_name</th>\n",
       "      <th>material_lvl2_name</th>\n",
       "      <th>material_lvl3_name</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>date</th>\n",
       "      <th>volume</th>\n",
       "      <th>ids</th>\n",
       "      <th>year</th>\n",
       "      <th>month_int</th>\n",
       "      <th>day</th>\n",
       "      <th>quarter</th>\n",
       "      <th>volume_per_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   material_code  company_code  country  region  manager_code      month  \\\n",
       "0              3             1       13      40            11 2018-01-01   \n",
       "1             11             1       12      34            34 2018-01-01   \n",
       "\n",
       "   material_lvl1_name  material_lvl2_name  material_lvl3_name  contract_type  \\\n",
       "0                   1                   4                   3              3   \n",
       "1                   1                   4                   3              3   \n",
       "\n",
       "        date  volume  ids  year  month_int  day  quarter  volume_per_month  \n",
       "0 2018-01-01    43.0    0  2018          1    1        1             192.0  \n",
       "1 2018-01-02    95.0    1  2018          1    2        1             145.0  "
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Создаем DataSet\n",
    "Он должен при инициализации:\n",
    "* Фильтровать данные при инициализации по месяцу\n",
    "* Принимать на вход длину последовательностей\n",
    "\n",
    "Метод Len:\n",
    "* длина уникальных ид клиентов если остановлюсь на таком варианте\n",
    "\n",
    "Метод Get_item\n",
    "* фильтрует по ИД группы\n",
    "* берет крайние N последовательностей\n",
    "* Если последовательностей меньше - паддит\n",
    "* Возращает на ИД группы словарь с тензорами  - Каждый тензор последовательность по фиче\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проблема в том, что там разные длины последовательностей\n",
    "\n",
    "Решение паддить и маскировать https://www.kdnuggets.com/2018/06/taming-lstms-variable-sized-mini-batches-pytorch.html\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "params = {\n",
    "    'features' : ['material_code', 'company_code', 'country', 'region', 'manager_code',\n",
    "       'material_lvl1_name', 'material_lvl2_name',\n",
    "       'material_lvl3_name', 'contract_type', 'volume',\n",
    "       'month_int', 'day', 'quarter', 'volume_per_month'],\n",
    "    'feature_to_emb' : ['material_code', 'company_code', 'country', 'region', 'manager_code',\n",
    "       'material_lvl1_name', 'material_lvl2_name',\n",
    "       'material_lvl3_name', 'contract_type',\n",
    "       'month_int', 'day', 'quarter'],\n",
    "    'dynamic_feat' : ['contract_type', 'volume',\n",
    "       'month_int', 'day', 'quarter'],\n",
    "    'static_feat' : ['material_code', 'company_code', 'country', 'region', 'manager_code',\n",
    "       'material_lvl1_name', 'material_lvl2_name',\n",
    "       'material_lvl3_name'],\n",
    "    'feat_cont': ['volume', 'volume_per_month'],\n",
    "    'seq_len':512, \n",
    "    'emb_params':{\"material_code\":(91,10,0),\n",
    "                  'company_code':(230,10,0),\n",
    "                  'country':(31,15,0),\n",
    "                  'region':(103,10,0),\n",
    "                  'manager_code':(60,20,0),\n",
    "                  'material_lvl1_name':(4,2,0),\n",
    "                  'material_lvl2_name':(6,3,0),\n",
    "                  'material_lvl3_name':(6,3,0),\n",
    "                  'contract_type':(4,2,0),\n",
    "                  'month_int':(13,6,0),\n",
    "                  'day':(32,10,0),\n",
    "                  'quarter':(5,2,0)},\n",
    "    'encoder_dropout':0.2,\n",
    "    'type_of_autoregressor':'LSTM',\n",
    "    'rnn_params':{'num_layers':2,\n",
    "        'hidden_size':128,\n",
    "        'bias':False\n",
    "    },\n",
    "    'predictor_params': {\n",
    "    'predictor_output':1,\n",
    "    'predictor_dropout':0.4\n",
    "    },\n",
    "    'lr': 0.03,\n",
    "    'epochs':1,\n",
    "    'weight_decay':0.01\n",
    "\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "class RnnDataset(Dataset):\n",
    "    def __init__(self, df, month, params, is_train=True) -> None:\n",
    "        self.df = df.loc[df.date<month].copy()\n",
    "        self.seq_len = params['seq_len']\n",
    "        self.params = params\n",
    "        self.is_train = is_train\n",
    "        if is_train:\n",
    "            self.targets= df_p[df_p.month == month].groupby('ids').agg({'volume':'sum'}).to_dict()['volume']\n",
    "        self.ind2ind =  {i:ind for i, ind in enumerate(self.df.ids.unique())}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.ids.nunique()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        features = self.params['features']\n",
    "        index = self.ind2ind[index]\n",
    "        df = self.df.loc[self.df.ids ==index][-self.seq_len:]\n",
    "\n",
    "        res = {\n",
    "            feat: torch.from_numpy(df[feat].values) for feat in features\n",
    "        }\n",
    "        \n",
    "        length = df.shape[0]\n",
    "        group_id = index\n",
    "        if self.is_train:\n",
    "            target = self.targets.get(index,0)\n",
    "\n",
    "            return group_id, res, length, target,\n",
    "            \n",
    "        return group_id, res, length, \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "class CollatorLearn(object):\n",
    "    def __init__(self, params, with_targets=False):\n",
    "        self.params = params\n",
    "        self.with_targets = with_targets\n",
    "    def __call__(self, batch):\n",
    "        #params = self.params\n",
    "        #print(params)\n",
    "        columns = self.params['features']\n",
    "        #columns = ['amount_rur', 'trans_date', 'small_group']\n",
    "        labels=[]\n",
    "        features={feat:[] for feat in columns}\n",
    "        lenghts=[]\n",
    "        \n",
    "        if self.with_targets:\n",
    "            targets=[]\n",
    "            for (_label, _features, _length, _target) in batch:\n",
    "                labels.append(_label)\n",
    "                lenghts.append(_length)\n",
    "                targets.append(_target)\n",
    "                for feat in columns:\n",
    "                    features[feat].append(_features[feat])\n",
    "            \n",
    "            res_dict = {\n",
    "                'labels' :torch.tensor(labels, dtype=torch.long),\n",
    "                'lenghts': torch.tensor(lenghts, dtype=torch.long),\n",
    "                'targets': torch.tensor(targets, dtype=torch.float)}\n",
    "            for feat in columns:\n",
    "                if feat in self.params['feat_cont']:\n",
    "                    res_dict[feat]= pad_sequence(features[feat], batch_first=True, padding_value=0).float()\n",
    "                else:\n",
    "                    res_dict[feat]= pad_sequence(features[feat], batch_first=True, padding_value=0)\n",
    "            \n",
    "            return res_dict\n",
    "        \n",
    "        #if NO Targets\n",
    "        for (_label, _features, _length) in batch:\n",
    "            labels.append(_label)\n",
    "            lenghts.append(_length)\n",
    "            for feat in columns:\n",
    "                features[feat].append(_features[feat])\n",
    "        \n",
    "        res_dict = {\n",
    "            'labels' :torch.tensor(labels,  dtype=torch.long),\n",
    "            'lenghts': torch.tensor(lenghts,  dtype=torch.long),\n",
    "            }\n",
    "        for feat in columns:\n",
    "            if feat in self.params['feat_cont']:\n",
    "                res_dict[feat]= pad_sequence(features[feat], batch_first=True, padding_value=0).float()\n",
    "            else:\n",
    "                res_dict[feat]= pad_sequence(features[feat], batch_first=True, padding_value=0)\n",
    "        \n",
    "        return res_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, params, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.params = params\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(*params['emb_params'][feat]).to(device) for feat in params['emb_params']])\n",
    "        input_size = len(params['feat_cont'])+sum([params['emb_params'][feat][1]for feat in params['emb_params']])\n",
    "        self.batch_norm = nn.BatchNorm1d(input_size).to(device)\n",
    "        self.encoder_dropout = nn.Dropout(params['encoder_dropout']).to(device)\n",
    "\n",
    "    def forward(self,x):\n",
    "        emb_out = [self.embeddings[i](x[feat]) for i, feat in enumerate(self.params['feature_to_emb'])]\n",
    "        cont_out = [x[feat].unsqueeze(-1) for feat in self.params['feat_cont']]\n",
    "        conc = torch.cat(cont_out+emb_out, dim=-1)\n",
    "        bnd = self.batch_norm(conc.permute(0,2,1)).permute(0,2,1)\n",
    "        drop = self.encoder_dropout(bnd)\n",
    "\n",
    "        #shape output Batch_size, Max_len per batch, Input_size\n",
    "\n",
    "        return drop"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "class Autoregressor(nn.Module):\n",
    "    def __init__(self, params, device):\n",
    "        super(Autoregressor, self).__init__()\n",
    "        input_size = len(params['feat_cont'])+sum([params['emb_params'][feat][1]for feat in params['emb_params']])\n",
    "        self.params=params\n",
    "        if self.params['type_of_autoregressor'] =='GRU':\n",
    "            self.autoreg = nn.GRU(input_size, hidden_size=params['rnn_params']['hidden_size'],\n",
    "                              num_layers = params['rnn_params']['num_layers'],\n",
    "                              bias = params['rnn_params']['bias'],\n",
    "                              batch_first=True, bidirectional=False).to(device)\n",
    "        if self.params['type_of_autoregressor'] =='LSTM':\n",
    "            self.autoreg = nn.LSTM(input_size, hidden_size=params['rnn_params']['hidden_size'],\n",
    "                              num_layers = params['rnn_params']['num_layers'],\n",
    "                              bias = params['rnn_params']['bias'],\n",
    "                              batch_first=True, bidirectional=False).to(device)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = pack_padded_sequence(x,lengths=lengths, batch_first=True, enforce_sorted=False)\n",
    "        if self.params['type_of_autoregressor']=='GRU':\n",
    "            rnn_d, h = self.autoreg(x)\n",
    "            #return torch.mean(rnn_d, 1)\n",
    "            \n",
    "            #shape output Batch_size, hidden_size\n",
    "            \n",
    "            return h[-1]\n",
    "        if self.params['type_of_autoregressor']=='LSTM':\n",
    "            rnn_d, (h,_) = self.autoreg(x)\n",
    "\n",
    "            return h[-1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, params, device):\n",
    "        super(Predictor, self).__init__()\n",
    "        input_size = params['rnn_params']['hidden_size']\n",
    "        output_size = params['predictor_params']['predictor_output']\n",
    "        self.FF1 = nn.Linear(input_size,input_size).to(device)\n",
    "        self.batch_norm = nn.BatchNorm1d(input_size).to(device)\n",
    "        self.predictor_dropout = nn.Dropout(params['predictor_params']['predictor_dropout']).to(device)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.FF2 = nn.Linear(input_size, output_size).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.FF1(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = self.predictor_dropout(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.FF2(out)\n",
    "\n",
    "        # shape output Batch_size, 1\n",
    "        out = out.view(-1)\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def loss_fn(criterion, preds, target):\n",
    "    #preds[preds<0]=0\n",
    "    loss = criterion(preds, np.log1p(target))\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def val_fn(model1, model2, model3, dataloader, device, criterion, params):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    model3.eval()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    columns = list(set(params['feature_to_emb']+params['feat_cont']))\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(dataloader), desc='batch', leave=False, position=1):\n",
    "            for feat in columns:\n",
    "                batch[feat].to(device, non_blocking=True)\n",
    "            out = model1(batch)\n",
    "            out2 = model2(out, batch['lenghts'])\n",
    "            out3= model3(out2)\n",
    "            loss = loss_fn(criterion, out3, batch['targets'])\n",
    "            losses.append(loss.detach().cpu().tolist())\n",
    "            targets.extend(batch['targets'].detach().cpu().tolist())\n",
    "            preds.extend(out3.detach().cpu().tolist())\n",
    "            writer.add_scalar('Loss/Val', loss, i)\n",
    "\n",
    "        return np.mean(losses), np.sqrt(msle(targets, (np.expm1(np.array(preds))).clip(min=0)))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def train_fn(model1, model2, model3, dataloader, optimizer, device, criterion, params):\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    model3.train()\n",
    "    clamp_class = Clamp()\n",
    "    preds = []\n",
    "    targets = []\n",
    "    columns = list(set(params['feature_to_emb']+params['feat_cont']))\n",
    "    losses = []\n",
    "    with torch.autograd.set_detect_anomaly(False):\n",
    "        for i, batch in tqdm(enumerate(dataloader), desc='batch', leave=False, position=1):\n",
    "            def closure():\n",
    "                loss = loss_fn(criterion, f_out, batch['targets'])\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for feat in columns:\n",
    "                batch[feat].to(device, non_blocking=True)\n",
    "            out = model1(batch)\n",
    "            out2 = model2(out, batch['lenghts'])\n",
    "            out3 = model3(out2)\n",
    "            clamp = clamp_class.apply\n",
    "            f_out = clamp(out3)\n",
    "            loss = loss_fn(criterion, f_out, batch['targets'])\n",
    "            #loss2 = loss_fn(MSLELoss(), f_out, batch['targets'])\n",
    "            #t_loss = loss2*0.7+0.3*loss\n",
    "            loss.backward()\n",
    "\n",
    "            #SAM\n",
    "            #optimizer.step(closure)\n",
    "            #loss_fn(criterion, f_out, batch['targets']).backward()\n",
    "            #optimizer.second_step(zero_grad=True)\n",
    "            \n",
    "            losses.append(loss.detach().cpu().tolist())\n",
    "            targets.extend(batch['targets'].detach().cpu().tolist())\n",
    "            preds.extend(f_out.detach().cpu().tolist())\n",
    "            writer.add_scalar('Loss/Train', loss, i)\n",
    "            #print(preds)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model2.parameters(), 1.5)\n",
    "            torch.nn.utils.clip_grad_norm_(model1.parameters(), 1.5)\n",
    "            torch.nn.utils.clip_grad_norm_(model3.parameters(), 1.5)\n",
    "            optimizer.step()\n",
    "\n",
    "        #return np.mean(losses), msle(date_prep.target_scaler.inverse_transform(np.array(targets).reshape(-1,1)), date_prep.target_scaler.inverse_transform(np.array(preds).reshape(-1,1)))\n",
    "        #return np.mean(losses), targets, preds\n",
    "        return np.mean(losses), np.sqrt(msle(targets, (np.expm1(np.array(preds))).clip(min=0)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "collate_test = CollatorLearn(params, with_targets=False)\n",
    "collate_train = CollatorLearn(params, with_targets=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "train_dataset = RnnDataset(df_p, '2020-01-01', params, is_train=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "test_dl = DataLoader(train_dataset, batch_size=2, collate_fn=collate_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "source": [
    "model = Encoder(params, device = torch.device('cpu'))\n",
    "model2 = Autoregressor(params, device= torch.device('cpu'))\n",
    "model3 = Predictor(params, device=torch.device('cpu'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "dd = next(iter(test_dl))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "def train_loop_fn(device, params):\n",
    "    criterion = nn.MSELoss()\n",
    "    #criterion = RMSLELoss()\n",
    "    criterion.to(device)\n",
    "\n",
    "    \n",
    "    encoder = Encoder(params, device=device)\n",
    "    context_encoder = Autoregressor(params, device=device)\n",
    "    predictor = Predictor(params, device=device)\n",
    "    #models_params = list(encoder.parameters()) + list(context_encoder.parameters())+ list(predictor.parameters())\n",
    "    #base_optimizer = torch.optim.AdamW(models_params, lr = params['lr'], weight_decay=params['weight_decay'])\n",
    "    #base_optimizer = torch.optim.AdamW\n",
    "    #optimizer = SAM(models_params, base_optimizer=base_optimizer, lr=params['lr'])\n",
    "    optimizer = torch.optim.AdamW([{'params': encoder.parameters()},\n",
    "                                   {'params': context_encoder.parameters()},\n",
    "                                   {'params': predictor.parameters()}], lr = params['lr'], weight_decay=params['weight_decay'], )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "    train_range = pd.date_range(start='2020-03-01', end='2020-06-01', freq='MS')\n",
    "    val_range = pd.date_range(start='2020-07-01', end='2020-07-01', freq='MS')\n",
    "    collate_train = CollatorLearn(params, with_targets=True)\n",
    "    print('Train it')\n",
    "    for j in tqdm(range(4), total=4, leave=True, position=0):\n",
    "        prokhod_loss=[]\n",
    "        prokhod_rmsle=[]\n",
    "        for next_month in tqdm(train_range, total=len(list(train_range)), leave=False):\n",
    "            #print(f'Train for next_month = {next_month}')\n",
    "            train_dataset = RnnDataset(df_p, next_month, params, is_train=True)\n",
    "            dataloader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_train, drop_last=True)\n",
    "\n",
    "            for epoch in range(1):\n",
    "                losses, msle_mean = train_fn(encoder, context_encoder, predictor, dataloader, optimizer, device, criterion, params)\n",
    "            #    losses, targets, preds = train_fn(encoder, context_encoder, predictor, dataloader, optimizer, device, criterion, params)\n",
    "            #print(f'Epoch №{epoch+1}: loss = {losses}, rmsle = {msle_mean}')\n",
    "            prokhod_loss.append(losses)\n",
    "            prokhod_rmsle.append(msle_mean)\n",
    "        print(f'Prokhod №{j+1}: loss = {np.mean(prokhod_loss)}, rmsle = {np.mean(prokhod_rmsle)}')\n",
    "        scheduler.step()\n",
    "       #return targets, preds\n",
    "        print('<<<<<<<<<<<--------------Validate it-------------->>>>>>>>>>>>>>')\n",
    "        for next_month in val_range:\n",
    "            #print(f'Val for next_month = {next_month}')\n",
    "            val_dataset = RnnDataset(df_p, next_month, params, is_train=True)\n",
    "            val_dataloader = DataLoader(val_dataset, batch_size=128, collate_fn=collate_train, drop_last=False)\n",
    "\n",
    "            \n",
    "            losses, msle_mean = val_fn(encoder, context_encoder, predictor, val_dataloader, device, criterion, params)\n",
    "            #    losses, targets, preds = train_fn(encoder, context_encoder, predictor, dataloader, optimizer, device, criterion, params)\n",
    "            print(f'Month {next_month}: loss = {losses}, rmsle = {msle_mean}')\n",
    "        \n",
    "    return encoder, context_encoder, predictor\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "encoder, context_encoder, predictor = train_loop_fn(torch.device('cpu'), params)\n",
    "#writer.flush()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train it\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/4 [04:07<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k_/t189wd911690dznlkp4xwd7h0000gn/T/ipykernel_80821/3808242204.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#writer.flush()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/k_/t189wd911690dznlkp4xwd7h0000gn/T/ipykernel_80821/1072735886.py\u001b[0m in \u001b[0;36mtrain_loop_fn\u001b[0;34m(device, params)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsle_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m#    losses, targets, preds = train_fn(encoder, context_encoder, predictor, dataloader, optimizer, device, criterion, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#print(f'Epoch №{epoch+1}: loss = {losses}, rmsle = {msle_mean}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/k_/t189wd911690dznlkp4xwd7h0000gn/T/ipykernel_80821/2081783524.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model1, model2, model3, dataloader, optimizer, device, criterion, params)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m#loss2 = loss_fn(MSLELoss(), f_out, batch['targets'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m#t_loss = loss2*0.7+0.3*loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m#SAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sibur-VtGsUYL5-py3.8/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/sibur-VtGsUYL5-py3.8/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "source": [
    "t, p = train_loop_fn(torch.device('cpu'), params)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train for next_month = 2019-05-01 00:00:00\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/12 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "source": [
    "\n",
    "torch.save(encoder.state_dict(), PATH_TO_ENCODER_WEIGHTS)\n",
    "torch.save(context_encoder.state_dict(), PATH_TO_RNN_WEIGHTS)\n",
    "torch.save(predictor.state_dict(), PATH_TO_FF_WEIGHTS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "source": [
    "PATH_TO_ENCODER_WEIGHTS = 'encoder_w.pt'\n",
    "PATH_TO_RNN_WEIGHTS = 'RNN_w.pt'\n",
    "PATH_TO_FF_WEIGHTS = 'FF_w.pt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "df_p"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>material_code</th>\n",
       "      <th>company_code</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>manager_code</th>\n",
       "      <th>month</th>\n",
       "      <th>material_lvl1_name</th>\n",
       "      <th>material_lvl2_name</th>\n",
       "      <th>material_lvl3_name</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>date</th>\n",
       "      <th>volume</th>\n",
       "      <th>ids</th>\n",
       "      <th>year</th>\n",
       "      <th>month_int</th>\n",
       "      <th>day</th>\n",
       "      <th>quarter</th>\n",
       "      <th>volume_per_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92301</th>\n",
       "      <td>14</td>\n",
       "      <td>46</td>\n",
       "      <td>17</td>\n",
       "      <td>102</td>\n",
       "      <td>39</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>12.0</td>\n",
       "      <td>636</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92302</th>\n",
       "      <td>42</td>\n",
       "      <td>181</td>\n",
       "      <td>17</td>\n",
       "      <td>81</td>\n",
       "      <td>13</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>62.0</td>\n",
       "      <td>59</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>723.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92303</th>\n",
       "      <td>75</td>\n",
       "      <td>225</td>\n",
       "      <td>17</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>20.0</td>\n",
       "      <td>875</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92304</th>\n",
       "      <td>80</td>\n",
       "      <td>143</td>\n",
       "      <td>29</td>\n",
       "      <td>95</td>\n",
       "      <td>31</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>25.0</td>\n",
       "      <td>926</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92305</th>\n",
       "      <td>18</td>\n",
       "      <td>162</td>\n",
       "      <td>16</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>25.0</td>\n",
       "      <td>907</td>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92306 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       material_code  company_code  country  region  manager_code      month  \\\n",
       "0                  3             1       13      40            11 2018-01-01   \n",
       "1                 11             1       12      34            34 2018-01-01   \n",
       "2                 74            41       10       4             5 2018-01-01   \n",
       "3                  3             1       13      40            11 2018-01-01   \n",
       "4                  2             1       12      34            45 2018-01-01   \n",
       "...              ...           ...      ...     ...           ...        ...   \n",
       "92301             14            46       17     102            39 2020-07-01   \n",
       "92302             42           181       17      81            13 2020-07-01   \n",
       "92303             75           225       17      45            19 2020-07-01   \n",
       "92304             80           143       29      95            31 2020-07-01   \n",
       "92305             18           162       16      55            38 2020-07-01   \n",
       "\n",
       "       material_lvl1_name  material_lvl2_name  material_lvl3_name  \\\n",
       "0                       1                   4                   3   \n",
       "1                       1                   4                   3   \n",
       "2                       1                   2                   2   \n",
       "3                       1                   4                   3   \n",
       "4                       1                   4                   3   \n",
       "...                   ...                 ...                 ...   \n",
       "92301                   3                   5                   5   \n",
       "92302                   1                   2                   2   \n",
       "92303                   1                   2                   2   \n",
       "92304                   1                   2                   2   \n",
       "92305                   1                   2                   2   \n",
       "\n",
       "       contract_type       date  volume  ids  year  month_int  day  quarter  \\\n",
       "0                  3 2018-01-01    43.0    0  2018          1    1        1   \n",
       "1                  3 2018-01-02    95.0    1  2018          1    2        1   \n",
       "2                  2 2018-01-02    57.0    2  2018          1    2        1   \n",
       "3                  3 2018-01-02    21.0    0  2018          1    2        1   \n",
       "4                  3 2018-01-02   150.0    3  2018          1    2        1   \n",
       "...              ...        ...     ...  ...   ...        ...  ...      ...   \n",
       "92301              1 2020-07-31    12.0  636  2020          7   31        3   \n",
       "92302              2 2020-07-31    62.0   59  2020          7   31        3   \n",
       "92303              2 2020-07-31    20.0  875  2020          7   31        3   \n",
       "92304              2 2020-07-31    25.0  926  2020          7   31        3   \n",
       "92305              2 2020-07-31    25.0  907  2020          7   31        3   \n",
       "\n",
       "       volume_per_month  \n",
       "0                 192.0  \n",
       "1                 145.0  \n",
       "2                 394.0  \n",
       "3                 192.0  \n",
       "4                1500.0  \n",
       "...                 ...  \n",
       "92301             206.0  \n",
       "92302             723.0  \n",
       "92303              60.0  \n",
       "92304             328.0  \n",
       "92305             197.0  \n",
       "\n",
       "[92306 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Тут Catboost в пайплайн пытаюсь вкорячить"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "from catboost import CatBoostRegressor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "group_ts = df_p.groupby(AGG_COLS +['ids']+[\"month\"])[\"volume\"].sum().unstack(fill_value=0)\n",
    "#group_ts = ids2group(group_ts.reset_index())\n",
    "#group_ts = group_ts.set_index(AGG_COLS)\n",
    "#group_ts = group_ts[['ids']+list(group_ts.columns[:-1])]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def get_features(df: pd.DataFrame, month: pd.Timestamp) -> pd.DataFrame:\n",
    "    \"\"\"Calculate features for `month`.\"\"\"\n",
    "\n",
    "    start_period = month - pd.offsets.MonthBegin(6)\n",
    "    end_period = month - pd.offsets.MonthBegin(1)\n",
    "\n",
    "    df = df.loc[:, :end_period]\n",
    "\n",
    "    features = pd.DataFrame([], index=df.index)\n",
    "    features[\"month\"] = month.month\n",
    "    #features['ids'] = df['ids']\n",
    "    features[[f\"vol_tm{i}\" for i in range(6, 0, -1)]] = df.loc[:, start_period:end_period].copy()\n",
    "\n",
    "    rolling = df.rolling(12, axis=1, min_periods=1)\n",
    "    features = features.join(rolling.mean().iloc[:, -1].rename(\"last_year_avg\"))\n",
    "    features = features.join(rolling.min().iloc[:, -1].rename(\"last_year_min\"))\n",
    "    features = features.join(rolling.max().iloc[:, -1].rename(\"last_year_max\"))\n",
    "    features = features.join(rolling.median().iloc[:, -1].rename(\"last_year_median\"))\n",
    "    features[\"month\"] = month.month\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "tr_range = pd.date_range(\"2020-03-01\", \"2020-06-01\", freq=\"MS\")\n",
    "#val_range = pd.date_range(\"2019-07-01\", \"2019-12-01\", freq=\"MS\")\n",
    "#ts_range = pd.date_range(\"2020-01-01\", \"2020-07-01\", freq=\"MS\")\n",
    "val_range = pd.date_range(\"2020-07-01\", \"2020-07-01\", freq=\"MS\")\n",
    "ts_range = pd.date_range(\"2020-01-01\", \"2020-07-01\", freq=\"MS\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "full_features = {}\n",
    "\n",
    "for dataset, dataset_range in zip([\"tr\", \"val\", \"ts\"], [tr_range, val_range, ts_range]):\n",
    "    dataset_features = []\n",
    "    for target_month in dataset_range:\n",
    "        features = get_features(group_ts, target_month)\n",
    "        features[\"target\"] = group_ts[target_month]\n",
    "        dataset_features.append(features.reset_index())\n",
    "    full_features[dataset] = pd.concat(dataset_features, ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "CAT_COLS = [\"material_code\", \"company_code\", \"country\", \"region\", \"manager_code\", \"month\"]\n",
    "FTS_COLS = [\"material_code\", \"company_code\", \"country\", \"region\", \"manager_code\", \"month\", \n",
    "            \"vol_tm6\", \"vol_tm5\", \"vol_tm4\", \n",
    "            \"vol_tm3\", \"vol_tm2\", \"vol_tm1\", \n",
    "            \"last_year_avg\", \n",
    "            \"last_year_min\", \n",
    "            \"last_year_max\",\n",
    "            #\"last_year_median\"\n",
    "            ]\n",
    "TARGET = \"target\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "model = CatBoostRegressor(iterations=600,\n",
    "                          early_stopping_rounds=150,\n",
    "                          learning_rate=0.009,\n",
    "                          #loss_function ='Tweedie:variance_power=1.4',\n",
    "                          #MSE, MultiRMSE, SurvivalAft, MAE, Quantile, LogLinQuantile, Poisson, MAPE, Lq \n",
    "                          #loss_function ='SurvivalAft',\n",
    "                          depth=4,\n",
    "                          cat_features=CAT_COLS,\n",
    "                          random_state=333,\n",
    "                          verbose=50)\n",
    "\n",
    "model.fit(full_features[\"tr\"][FTS_COLS], np.log1p(full_features[\"tr\"][TARGET]),\n",
    "          eval_set=(full_features[\"val\"][FTS_COLS], np.log1p(full_features[\"val\"][TARGET]))\n",
    "          )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0:\tlearn: 2.2718969\ttest: 2.3302482\tbest: 2.3302482 (0)\ttotal: 54.8ms\tremaining: 32.8s\n",
      "50:\tlearn: 2.0142869\ttest: 2.0234176\tbest: 2.0234176 (50)\ttotal: 154ms\tremaining: 1.66s\n",
      "100:\tlearn: 1.8760824\ttest: 1.8425487\tbest: 1.8425487 (100)\ttotal: 242ms\tremaining: 1.2s\n",
      "150:\tlearn: 1.8039681\ttest: 1.7419432\tbest: 1.7419432 (150)\ttotal: 326ms\tremaining: 968ms\n",
      "200:\tlearn: 1.7643247\ttest: 1.6826129\tbest: 1.6826129 (200)\ttotal: 406ms\tremaining: 805ms\n",
      "250:\tlearn: 1.7381793\ttest: 1.6457485\tbest: 1.6457485 (250)\ttotal: 486ms\tremaining: 675ms\n",
      "300:\tlearn: 1.7190786\ttest: 1.6212338\tbest: 1.6212338 (300)\ttotal: 576ms\tremaining: 572ms\n",
      "350:\tlearn: 1.7051307\ttest: 1.6049976\tbest: 1.6049976 (350)\ttotal: 675ms\tremaining: 479ms\n",
      "400:\tlearn: 1.6943600\ttest: 1.5926992\tbest: 1.5926447 (399)\ttotal: 764ms\tremaining: 379ms\n",
      "450:\tlearn: 1.6858805\ttest: 1.5857326\tbest: 1.5857326 (450)\ttotal: 899ms\tremaining: 297ms\n",
      "500:\tlearn: 1.6786129\ttest: 1.5801611\tbest: 1.5799187 (499)\ttotal: 1.04s\tremaining: 206ms\n",
      "550:\tlearn: 1.6725049\ttest: 1.5751862\tbest: 1.5751862 (550)\ttotal: 1.22s\tremaining: 108ms\n",
      "599:\tlearn: 1.6676090\ttest: 1.5719556\tbest: 1.5719556 (599)\ttotal: 1.35s\tremaining: 0us\n",
      "\n",
      "bestTest = 1.571955558\n",
      "bestIteration = 599\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x7ff159558f10>"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "full_features['tr']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>material_code</th>\n",
       "      <th>company_code</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>manager_code</th>\n",
       "      <th>ids</th>\n",
       "      <th>month</th>\n",
       "      <th>vol_tm6</th>\n",
       "      <th>vol_tm5</th>\n",
       "      <th>vol_tm4</th>\n",
       "      <th>vol_tm3</th>\n",
       "      <th>vol_tm2</th>\n",
       "      <th>vol_tm1</th>\n",
       "      <th>last_year_avg</th>\n",
       "      <th>last_year_min</th>\n",
       "      <th>last_year_max</th>\n",
       "      <th>last_year_median</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>17</td>\n",
       "      <td>64</td>\n",
       "      <td>42</td>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>607</td>\n",
       "      <td>3</td>\n",
       "      <td>124.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>102.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>648</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>5</td>\n",
       "      <td>526</td>\n",
       "      <td>3</td>\n",
       "      <td>83.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>17</td>\n",
       "      <td>548</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>29.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>89</td>\n",
       "      <td>228</td>\n",
       "      <td>17</td>\n",
       "      <td>75</td>\n",
       "      <td>42</td>\n",
       "      <td>933</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>24.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>706</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>25</td>\n",
       "      <td>820</td>\n",
       "      <td>6</td>\n",
       "      <td>29.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>93.166667</td>\n",
       "      <td>15.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>108.5</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3762</th>\n",
       "      <td>90</td>\n",
       "      <td>109</td>\n",
       "      <td>17</td>\n",
       "      <td>102</td>\n",
       "      <td>25</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>123.333333</td>\n",
       "      <td>80.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3763</th>\n",
       "      <td>90</td>\n",
       "      <td>135</td>\n",
       "      <td>17</td>\n",
       "      <td>58</td>\n",
       "      <td>3</td>\n",
       "      <td>824</td>\n",
       "      <td>6</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3764 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      material_code  company_code  country  region  manager_code  ids  month  \\\n",
       "0                 1           161       17      64            42   72      3   \n",
       "1                 2             1        2      42             5  607      3   \n",
       "2                 2             1        2      43             5  648      3   \n",
       "3                 2             1        2      99             5  526      3   \n",
       "4                 2             1       10     101            17  548      3   \n",
       "...             ...           ...      ...     ...           ...  ...    ...   \n",
       "3759             89           228       17      75            42  933      6   \n",
       "3760             90             1       17      38            48  706      6   \n",
       "3761             90            70       17      38            25  820      6   \n",
       "3762             90           109       17     102            25   43      6   \n",
       "3763             90           135       17      58             3  824      6   \n",
       "\n",
       "      vol_tm6  vol_tm5  vol_tm4  vol_tm3  vol_tm2  vol_tm1  last_year_avg  \\\n",
       "0         0.0      0.0      0.0      0.0      0.0      0.0      45.833333   \n",
       "1       124.0    181.0    208.0    207.0     17.0     72.0     102.666667   \n",
       "2         0.0      0.0      0.0      0.0      0.0      0.0      32.500000   \n",
       "3        83.0     82.0     42.0      0.0      0.0      0.0      20.666667   \n",
       "4         0.0     45.0     50.0     45.0      0.0     50.0      29.166667   \n",
       "...       ...      ...      ...      ...      ...      ...            ...   \n",
       "3759      0.0      0.0     21.0     63.0    125.0     84.0      24.416667   \n",
       "3760      6.0      5.0      5.0      5.0      0.0      3.0       5.333333   \n",
       "3761     29.0     73.0     74.0    122.0    100.0     15.0      93.166667   \n",
       "3762    100.0    100.0    180.0    180.0    100.0    140.0     123.333333   \n",
       "3763     40.0     40.0      0.0     20.0     40.0     20.0      23.333333   \n",
       "\n",
       "      last_year_min  last_year_max  last_year_median  target  \n",
       "0               0.0          200.0               0.0     0.0  \n",
       "1               0.0          208.0              90.0   250.0  \n",
       "2               0.0          145.0               0.0     0.0  \n",
       "3               0.0           83.0               0.0     0.0  \n",
       "4               0.0           50.0              40.0    40.0  \n",
       "...             ...            ...               ...     ...  \n",
       "3759            0.0          125.0               0.0    84.0  \n",
       "3760            0.0           10.0               5.0     3.0  \n",
       "3761           15.0          129.0             108.5    30.0  \n",
       "3762           80.0          180.0             120.0    40.0  \n",
       "3763            0.0           40.0              20.0    21.0  \n",
       "\n",
       "[3764 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from lightgbm import LGBMRegressor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hyper_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': ['l1','l2'],\n",
    "    'learning_rate': 0.005,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 10,\n",
    "    #'verbose': 0,\n",
    "    \"max_depth\": 8,\n",
    "    \"num_leaves\": 128,  \n",
    "    \"max_bin\": 512,\n",
    "    \"num_iterations\": 100000\n",
    "}"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Kernel is dead",
     "traceback": [
      "Error: Kernel is dead",
      "at g._sendKernelShellControl (/Users/ltorrick/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:1006195)",
      "at g.sendShellMessage (/Users/ltorrick/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:1005964)",
      "at g.requestExecute (/Users/ltorrick/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:1008506)",
      "at d.requestExecute (/Users/ltorrick/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:37:325680)",
      "at w.requestExecute (/Users/ltorrick/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:32:18027)",
      "at w.executeCodeCell (/Users/ltorrick/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:301076)",
      "at w.execute (/Users/ltorrick/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:300703)",
      "at w.start (/Users/ltorrick/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:296367)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/Users/ltorrick/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:311160)",
      "at async t.CellExecutionQueue.start (/Users/ltorrick/.vscode/extensions/ms-toolsai.jupyter-2021.8.2041215044/out/client/extension.js:52:310700)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "gbm = LGBMRegressor(**hyper_params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "gbm.fit(full_features[\"tr\"][FTS_COLS], np.log1p(full_features[\"tr\"][TARGET]),\n",
    "        eval_set=[(full_features[\"val\"][FTS_COLS], np.log1p(full_features[\"val\"][TARGET]))],\n",
    "        eval_metric='l2',\n",
    "        early_stopping_rounds=1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "dd.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'lenghts', 'targets', 'material_code', 'company_code', 'country', 'region', 'manager_code', 'material_lvl1_name', 'material_lvl2_name', 'material_lvl3_name', 'contract_type', 'volume', 'month_int', 'day', 'quarter', 'volume_per_month'])"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "dd['month_int']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
       "          3,  3,  4,  4,  4,  4,  5,  5,  5,  5,  5, 11, 11, 11, 11, 12, 12, 12,\n",
       "         12,  1,  1,  1,  1,  1,  2, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11,\n",
       "         11, 11, 11, 11, 12, 12, 12, 12, 12, 12],\n",
       "        [ 1,  1,  2,  3,  4,  7,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6rc1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6rc1 64-bit ('sibur-VtGsUYL5-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "7ce1d418a45594d344f0eb27a8d7fb82238d509323320e10f97cd5c6df2cef09"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}